= 301 Project 01 - Choosing a Model

== Project Objectives

In this project, we will learn how to select an appropriate machine learning model. Understanding specifics of how the models work may help in this process, but other aspects can be investigated for this. 

== Extra Reading and Resources

- https://the-examples-book.com/starter-guides/data-science/data-modeling/choosing-model/[DataMine Examples Book - Choosing a Model]

== Dataset
- `/anvil/projects/tdm/data/boston.csv`

== Questions

=== Question 1 (2 points)

One of the most important parts of picking a model is understanding your data. 

For this project, we will be using the boston dataset. Run the following code to load the dataset into a pandas DataFrame.
[source,python]
----
import pandas as pd
 
my_df = pd.read_csv('/anvil/projects/tdm/data/boston.csv')
----

Now that we have loaded the dataset, let's take a look at the dimensions of the dataset.

We can find the number of rows and columns in the dataset using the `shape` attribute of the DataFrame.
[source,python]
----
my_df.shape
----

.Deliverables
====
- The number of rows and columns in the dataset
====

=== Question 2 (2 points)

The first step of preprocessing data for a ML model is choosing what our output variable should be.

The output variable(s) help determine if we should use a regression or classification model. Typically, if the output variable is continuous, we use a regression model. If the output variable is categorical, we use a classification model.

For this project, we will use the `MEDV` column as our output variable.

Run the following code to separate the input features from the output variable
[source,python]
----
# Split the dataset into features and target variable
X = my_df.drop('MEDV', axis=1)   
y = my_df['MEDV']
----
[NOTE]
====
It is common convention to use `X` to represent the input features and `y` to represent the output variable. All example code in this and future projects will follow this convention.
====

Now that we have separated the input features and output variable, let's take a look at the first few rows of the output variable.

[source,python]
----
y.head()
----

.Deliverables
====
- What type of data is the MEDV column? (is it a continuous or categorical variable?)
====

=== Question 3 (2 points)

In order to evaluate how well the machine learning model is performing, we will split our dataset into training and testing sets.
The training set will be used to train the machine learning model, while the testing model will be used after training in order to analyze how well the model performs.

[IMPORTANT]
====
It is also common to split the data into training, validation, and testing sets. The validation set is used during training to help tune the hyperparameters of the model. For this project, we will only use training and testing sets.
====

scikit-learn is a great python library for machine learning, and you will be using it all the time during these projects. Reading through https://scikit-learn.org/stable/user_guide.html[their documentation] may help you.

Run the following code to split the data into training and testing sets
[source,python]
----
from sklearn.model_selection import train_test_split
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
----

Print the first few rows of the training data input features using the 'head' method. You may notice that each feature has a different range.

.Deliverables
====
- Which feature has the largest range? Which feature has the smallest range?
====

=== Question 4 (2 points)

As you saw in the previous question, the input features have different ranges. This can cause problems for some machine learning models.

Some models will assign more importance to features with larger ranges, which can lead to incorrect predictions.

To avoid this, we can scale the input features so that they all have the same range.

We can do this using scikit-learn's https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html[StandardScaler] class. Run the following code.
[source,python]
----
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
# scale the training data
X_train_scaled = scaler.fit_transform(X_train)
# scale the testing data
X_test_scaled = scaler.transform(X_test)
----
[IMPORTANT]
====
We use `fit_transform` on the training data and `transform` on the testing data. The fit_transform method simply runs the fit method and then the transform method.
The fit method calculates the mean and standard deviation of the data, while the transform method scales the data using the mean and standard deviation.
====

.Deliverables
====
- Please explain why we only fit the scaler on the training data and not the testing data.
====

=== Question 5 (2 points)

When deciding which model to use, it is important to consider different aspects of how we want the model to behave.
Some primary aspects of a model to consider are:

- Flexibility vs. Interpretability:
As model's become more flexible and able to handle complex data, they also become harder to interpret by becoming "black boxes".
It is important to consider how much you value the interpretability of the model.

- Classification vs. Regression:
We have briefly touched on this in question 2, but it is important to understand the difference between classification and regression problems.
Predicting housing prices, age, income, etc. would be regression problems, while predicting eye color, animal type, etc. would be a classification problem.

- Prediction vs. Inference:
A predictive model focuses on predicting future information, such as stock prices, while an inferential model focuses on understanding the relationships between variables.

- Supervised vs. Unsupervised Learning:
Supervised learning uses a labeled dataset to make predictions, while unsupervised learning discovers patterns in the data without labels.

- Parameterization vs. Non-Parameterization:
Parameterization involves assigning parameters to help develop a function, where as non-parameterization uses the data itself to derive function parameters.

.Deliverables
====
- Is our dataset more suitable for a regression or classification model? Why?
- Would our dataset be more suitable for supervised or unsupervised learning? Why?
====

Project 01 Assignment Checklist
====
* Jupyter Lab notebook with your code, comments, and output for the assignment
    ** `firstname-lastname-project01.ipynb` 

* Submit files through Gradescope
====

[WARNING]
====
_Please_ make sure to double-check that your submission is complete and contains all of your code and output before submitting. If you have a spotty internet connection, it is recommended to download your submission after submitting it to ensure what you _think_ you submitted is what you _actually_ submitted.

In addition, please review our https://the-examples-book.com/projects/submissions[submission guidelines] before submitting your project.
====
